{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import gc\n",
    "import time\n",
    "import torch.utils.data as D\n",
    "from torch import from_numpy\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from UNetModel import UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Custom dataset for the processed lung images:\n",
    "    - Takes in the corresponding meta csv file, nodule csv file, and processed img directory\n",
    "    - When getting an item (image), creates the label and returns \n",
    "      a dict with the image as a tensor and label as a tensor\n",
    "'''\n",
    "class LungsDataset(D.Dataset):\n",
    "    def __init__(self, meta, nodules, img_dir):\n",
    "        self.meta = meta.sample(frac = 1) # shuffles the data in a copy\n",
    "        self.cands = nodules.sample(frac = 1) # shuffles the data in a copy\n",
    "        self.img_dir = img_dir\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.meta)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.meta.iloc[[idx]]\n",
    "        \n",
    "        # meta information for the scan\n",
    "        name = str(row.iloc[0]['Name'])\n",
    "        originX = float(row.iloc[0]['OriginX'])\n",
    "        originY = float(row.iloc[0]['OriginY'])\n",
    "        originZ = float(row.iloc[0]['OriginZ'])\n",
    "        spacingX = float(row.iloc[0]['SpacingX'])\n",
    "        spacingY = float(row.iloc[0]['SpacingY'])\n",
    "        spacingZ = float(row.iloc[0]['SpacingZ'])\n",
    "\n",
    "        # nodules for each scan\n",
    "        nodules = self.cands[self.cands['seriesuid'] == name][['coordX', 'coordY', 'coordZ']]\n",
    "        nodules['coordX'] = ((nodules['coordX'] - originX)/spacingX).astype(int)\n",
    "        nodules['coordY'] = ((nodules['coordY'] - originY)/spacingY).astype(int)\n",
    "        nodules['coordZ'] = ((nodules['coordZ'] - originZ)/spacingZ).astype(int)\n",
    "        \n",
    "        # processed image (numpy array)\n",
    "        for file in os.listdir(self.img_dir):\n",
    "            if re.search(name + '.npy$', file):\n",
    "                img = np.load(self.img_dir + 'p_' + name + '.npy')\n",
    "                break\n",
    "                \n",
    "        # convert nodules to 1-hot\n",
    "        label = np.zeros(img.shape)\n",
    "        nodules = nodules.values\n",
    "        for ind in range(nodules.shape[0]):\n",
    "            nod = nodules[ind, :]\n",
    "            label[nod[2], nod[1], nod[0]] = 1\n",
    "        \n",
    "        # convert img, label into tensors\n",
    "        return from_numpy(img).unsqueeze(0).float(), from_numpy(label).int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function to call when initializing the data. \n",
    "VERY specific to this project's directory setup. processed9 is the TEST dataset, do NOT touch.\n",
    "Returns: the dataloaders for the img_dirs (training/CV) OR the dataloader for test img_dir\n",
    "'''\n",
    "def load_data(nodule_file, *img_dirs, load_training, shuffle = False, batch_size = 1):\n",
    "    if load_training:\n",
    "        fold_dataloaders = []\n",
    "        index = 0\n",
    "        nodules = pd.read_csv(nodule_file)\n",
    "        for img_dir in list(img_dirs):\n",
    "            meta_file = os.path.join(img_dir, 'meta_' + str(index) + '.csv')\n",
    "            meta = pd.read_csv(meta_file)\n",
    "            dataset = LungsDataset(meta, nodules, img_dir)\n",
    "            dataloader = D.DataLoader(dataset = dataset, batch_size = batch_size, shuffle = shuffle)\n",
    "            fold_dataloaders.append(dataloader)\n",
    "            index += 1\n",
    "        return fold_dataloaders\n",
    "    else:\n",
    "        img_dir = list(img_dirs)[0]\n",
    "        meta_file = os.path.join(img_dir, 'meta_9.csv')\n",
    "        meta = pd.read_csv(meta_file)\n",
    "        nodules = pd.read_csv(nodule_file)\n",
    "        dataset = LungsDataset(meta, nodules, img_dir)\n",
    "        test_dataloader = D.DataLoader(dataset = dataset, batch_size = 1, shuffle = False)\n",
    "        return test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Training function on the training/cv folds. Trains on all but one fold.\n",
    "'''\n",
    "\n",
    "def train(model, criterion, optimizer, device, fold_loaders, num_epoch, cv_fold):\n",
    "    model = model.to(device)\n",
    "    running_loss = 0.0\n",
    "    count = 0\n",
    "    for epoch in range(num_epoch):\n",
    "        running_loss = 0.0\n",
    "        count = 0\n",
    "        for fold, train_loader in enumerate(fold_loaders):\n",
    "            if cv_fold == fold:\n",
    "                continue\n",
    "            for i_batch, (imgs, labels) in enumerate(train_loader): \n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                imgs = imgs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                output = model(imgs)\n",
    "                loss = criterion(output, labels)\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                count += labels.size()[0]\n",
    "                \n",
    "                print('Batch ' + str(i_batch) + ' has completed. Batch loss: ' + str(loss.item() / labels.size()[0]) + \n",
    "                      '. Avg loss so far: ' + str(running_loss / count) + '. Running loss: ' + str(running_loss))\n",
    "        print('Avg loss for epoch ' + str(epoch) + ': ' + str(running_loss / count))\n",
    "    \n",
    "    return running_loss / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Evaluation function for CV or test. Batch size should be 1.\n",
    "'''\n",
    "def evaluate(model, criterion, loader):\n",
    "    running_loss = 0.0\n",
    "    count = 0\n",
    "    for i_batch, (imgs, labels) in enumerate(loader):\n",
    "        output = model(imgs)\n",
    "        loss = criterion(output, scores)\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        count += labels.size()[0]\n",
    "        print('Avg loss: ' + str(running_loss / count))\n",
    "    print('Done.')\n",
    "    \n",
    "    return running_loss / len(loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
