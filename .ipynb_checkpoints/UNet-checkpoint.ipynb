{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import SimpleITK as sitk\n",
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "from PIL import Image\n",
    "from sklearn.cluster import KMeans\n",
    "from skimage.morphology import binary_erosion, binary_dilation, binary_opening, binary_closing\n",
    "from skimage.measure import regionprops, label\n",
    "import re\n",
    "import pandas as pd\n",
    "import gc\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as D\n",
    "from torch import from_numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Partitions the folder downloaded into smaller ones, which is easier to handle.\n",
    "EX: ~/.../subset0/img.mhd -> ~/.../subset0/0/img.mhd\n",
    "n_files: max number of files in a folder (the /0/ maximum)\n",
    "'''\n",
    "\n",
    "def split_dir(d, n_files = 8):\n",
    "    lst_files = []\n",
    "    file_c = 0\n",
    "    c = 0\n",
    "    new_dir = d\n",
    "    for file in os.listdir(d):\n",
    "        if re.search('.mhd$', file) or re.search('.raw$', file):\n",
    "            lst_files.append(file)\n",
    "    for file in sorted(lst_files):\n",
    "        if c == 2 * n_files or file_c == 0: # EVEN to group the .raw and .mhd files in same directory\n",
    "            new_dir = d + str(file_c) + '/'\n",
    "            os.makedirs(new_dir, 0o755, exist_ok = True)\n",
    "            c = 0\n",
    "            file_c += 1\n",
    "        os.rename(d + file, new_dir + file)\n",
    "        c += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Initial processing functions.\n",
    "'''\n",
    "\n",
    "# load in SimpleITK .mhd file as numpy array\n",
    "def load_img(filename):\n",
    "    itkimage = sitk.ReadImage(filename)\n",
    "    numpy_image = sitk.GetArrayFromImage(itkimage) # z, y, x...\n",
    "    numpy_origin = np.array(itkimage.GetOrigin()) # x, y, z...\n",
    "    numpy_spacing = np.array(itkimage.GetSpacing())\n",
    "    return numpy_image, numpy_origin, numpy_spacing\n",
    "\n",
    "# normalize image\n",
    "def normalize(img):\n",
    "    mean = np.mean(img)\n",
    "    std = np.std(img)\n",
    "    return (img - mean)/std\n",
    "\n",
    "# show distribution of intensity\n",
    "def distribution(normalize_img):\n",
    "    plt.hist(normalize_img.flatten(), bins = 200)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Convert norm_img to an image (numpy array) with only 1s and 0s.\n",
    "1s = initial lungs regions + noise\n",
    "0s = background/non-lung tissue + noise\n",
    "PRECONDITION: norm_img is a normalized numpy array\n",
    "'''\n",
    "\n",
    "def convert_binary(norm_img):\n",
    "    # move outliers of images with the extra background circle to \n",
    "    # intensity that won't affect kmeans significantly\n",
    "    \n",
    "    # empirically determined\n",
    "    med = np.median(norm_img)\n",
    "    norm_img[norm_img == np.min(norm_img)] = med\n",
    "    norm_img[norm_img < -1.5 ] = med\n",
    "\n",
    "    print('Kmeans is doing its thing.')\n",
    "    # empirical values but haven't really tested thoroughly, kmeans takes too long\n",
    "    kmeans = KMeans(n_clusters = 2, random_state = 0, n_init = 1, max_iter = 1).fit(norm_img.flatten().reshape(-1, 1))\n",
    "    print('Kmeans is done.')\n",
    "    gc.collect()\n",
    "    \n",
    "    # convert image to 1s and 0s based on threshold\n",
    "    centers = np.sort(kmeans.cluster_centers_.flatten())\n",
    "    thresh = (centers[0] + centers[1]) / 2\n",
    "    bin_img = np.where(norm_img < thresh, 1.0, 0.0)\n",
    "    return bin_img  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Removes noise regions from bin_img through erosion and then dilation. \n",
    "PRECONDITION: bin_img is binary numpy array\n",
    "# '''\n",
    "\n",
    "def make_regions(bin_img):\n",
    "    for s in range(bin_img.shape[0]):\n",
    "        s1 = bin_img[s,:,:]\n",
    "        s2 = binary_erosion(s1, selem = np.ones([2, 2]))  # empirically determined\n",
    "        bin_img[s,:,:] = binary_dilation(s2, selem = np.ones([8, 8])) # empirically determined\n",
    "    return bin_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Isolates the lung regions and makes a binary mask over them.\n",
    "1s = lung regions\n",
    "0s = non-lung regions\n",
    "PRECONDITION: reg_img is a non-noisy binary image\n",
    "'''\n",
    "\n",
    "def make_mask(reg_img):\n",
    "    mask = np.zeros(reg_img.shape)\n",
    "    for s in range(reg_img.shape[0]):\n",
    "        # a slice in the 3D reg_img\n",
    "        s1 = reg_img[s,:,:]\n",
    "        props = regionprops(label(s1))\n",
    "        labels = []\n",
    "        for p in props:\n",
    "            B = p.bbox\n",
    "            # empirically determined, region has to be within confines and large enough\n",
    "            if (((B[0] > 3 and B[2] < 509) or (B[1] > 3 and \n",
    "                B[3] < 509)) and B[2] - B[0] > 25 and B[3] - B[1] > 25):\n",
    "                labels.append(p.label)\n",
    "        for l in labels:\n",
    "            mask[s, :, :] = mask[s, :, :] + np.where(label(s1) == l, 1, 0)\n",
    "        mask[s, :, :] = binary_dilation(mask[s, :, :], selem = np.ones([8, 8])) # empirically determined\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Applies the mask to the original, non-normalized image. \n",
    "Renormalizes using significant regions only, which is desired.\n",
    "PRECONDITION: mask is original's corresponding mask\n",
    "'''\n",
    "\n",
    "def mask_on_orig(original, mask):\n",
    "    new_img = original * mask\n",
    "    \n",
    "    mean = np.mean(original[mask > 0])\n",
    "    std = np.std(original[mask > 0])\n",
    "    new_img = (new_img - mean)/std\n",
    "    # set non-lung regions to -5, which is logical since it's highly unlikely to interfere\n",
    "    new_img[mask==0] = -5 \n",
    "    return new_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Uses previous methods to process through a whole image.\n",
    "Returns the masked image and (origin and spacing) from SimpleITK\n",
    "'''\n",
    "\n",
    "def process_img(filename):\n",
    "    img, origin, spacing = load_img(filename)\n",
    "    norm_img = normalize(img)\n",
    "    bin_img = convert_binary(norm_img)\n",
    "    reg_img = make_regions(bin_img)\n",
    "    mask = make_mask(reg_img)\n",
    "    masked_img = mask_on_orig(img, mask)\n",
    "    \n",
    "    # visualize a slice to make sure mask is generated correctly\n",
    "    plt.subplot(231)\n",
    "    plt.imshow(img[int(0.3 * img.shape[0]), 0:512, 0:512])\n",
    "    plt.subplot(232)\n",
    "    plt.imshow(img[int(0.5 * img.shape[0]), 0:512, 0:512])\n",
    "    plt.subplot(233)\n",
    "    plt.imshow(img[int(0.8 * img.shape[0]), 0:512, 0:512])\n",
    "    plt.subplot(234)\n",
    "    plt.imshow(masked_img[int(0.3 * masked_img.shape[0]), 0:512, 0:512])\n",
    "    plt.subplot(235)\n",
    "    plt.imshow(masked_img[int(0.5 * masked_img.shape[0]), 0:512, 0:512])\n",
    "    plt.subplot(236)\n",
    "    plt.imshow(masked_img[int(0.8 * masked_img.shape[0]), 0:512, 0:512])\n",
    "    plt.show()\n",
    "    \n",
    "    return masked_img, origin, spacing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Processes all LUNA files in a directory:\n",
    "    - creates a new directory on the same level as the directory of images\n",
    "    - uses process_img to get the masked image, origin, spacing for each image\n",
    "    - stores the masked image array into a text file in the new directory\n",
    "    - stores the origin, spacing coordinates, size of array into a csv file in out_meta\n",
    "'''\n",
    "\n",
    "def process_dir(d, out_dir, out_meta):\n",
    "    os.makedirs(out_dir, 0o755, exist_ok = True)\n",
    "    \n",
    "    originsZ = []\n",
    "    originsY = []\n",
    "    originsX = []\n",
    "    spacingsZ = []\n",
    "    spacingsY = []\n",
    "    spacingsX = []\n",
    "    names = []\n",
    "    zDim = []\n",
    "    yDim = []\n",
    "    xDim = []\n",
    "    df = pd.read_csv(out_meta)\n",
    "    \n",
    "    for file in os.listdir(d):\n",
    "        gc.collect()\n",
    "        if re.search('.mhd$', file):\n",
    "            print(file + ' is processing')\n",
    "            img, origin, spacing = process_img(d + file)\n",
    "            np.save(out_dir + 'p_' + re.split('.mhd$', file)[0], img)\n",
    "            gc.collect()\n",
    "            \n",
    "            z = img.shape[0]\n",
    "            y = img.shape[1]\n",
    "            x = img.shape[2]\n",
    "            \n",
    "            originsZ.append(origin[2])\n",
    "            originsY.append(origin[1])\n",
    "            originsX.append(origin[0])\n",
    "            spacingsZ.append(spacing[2])\n",
    "            spacingsY.append(spacing[1])\n",
    "            spacingsX.append(spacing[0])\n",
    "            zDim.append(z)\n",
    "            yDim.append(y)\n",
    "            xDim.append(x)\n",
    "            names.append(re.split('.mhd$', file)[0])\n",
    "                        \n",
    "    new = pd.DataFrame({'Name': names, 'OriginZ': originsZ, 'OriginY': originsY, 'OriginX': originsX,\n",
    "                        'SpacingZ': spacingsZ, 'SpacingY': spacingsY, 'SpacingX': spacingsX, \n",
    "                        'zDim': zDim, 'yDim': yDim, 'xDim': xDim})\n",
    "    df = pd.concat([df, new], axis = 0)\n",
    "    df.to_csv(out_meta, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_dir('/Volumes/KaneData/subset0/', \n",
    "            '/Volumes/KaneData/processed0/', \n",
    "            '/Volumes/KaneData/meta_0.csv')\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_dir('/Volumes/KaneData/subset1/', \n",
    "            '/Volumes/KaneData/processed1/', \n",
    "            '/Volumes/KaneData/meta_1.csv')\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_dir('/Volumes/KaneData/subset2/', \n",
    "            '/Volumes/KaneData/processed2/', \n",
    "            '/Volumes/KaneData/meta_2.csv')\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_dir('/Volumes/KaneData/subset3/', \n",
    "            '/Volumes/KaneData/processed3/', \n",
    "            '/Volumes/KaneData/meta_3.csv')\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_dir('/Volumes/KaneData/subset4/', \n",
    "            '/Volumes/KaneData/processed4/', \n",
    "            '/Volumes/KaneData/meta_4.csv')\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_dir('/Volumes/KaneData/subset5/', \n",
    "            '/Volumes/KaneData/processed5/', \n",
    "            '/Volumes/KaneData/meta_5.csv')\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_dir('/Volumes/KaneData/subset6/', \n",
    "            '/Volumes/KaneData/processed6/', \n",
    "            '/Volumes/KaneData/meta_6.csv')\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Custom dataset for the processed lung images:\n",
    "    - Takes in the corresponding meta csv file, nodule csv file, and processed img directory\n",
    "    - When getting an item (image), creates the label and returns \n",
    "      a dict with the image as a tensor and label as a tensor\n",
    "'''\n",
    "class LungsDataset(D.Dataset):\n",
    "    def __init__(self, meta_file, nodule_file, img_dir):\n",
    "        self.meta = pd.read_csv(meta_file)\n",
    "        self.cands = pd.read_csv(nodule_file)\n",
    "        self.img_dir = img_dir\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.meta)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.meta.iloc[0]\n",
    "        \n",
    "        # meta information for the scan\n",
    "        name = row['Name']\n",
    "        originX = row['OriginX']\n",
    "        originY = row['OriginY']\n",
    "        originZ = row['OriginZ']\n",
    "        spacingX = row['SpacingX']\n",
    "        spacingY = row['SpacingY']\n",
    "        spacingZ = row['SpacingZ']\n",
    "\n",
    "        # nodules for each scan\n",
    "        nodules = self.cands[self.cands['seriesuid'] == name][['coordX', 'coordY', 'coordZ']]\n",
    "        nodules['coordX'] = ((nodules['coordX'] - originX)/spacingX).astype(int)\n",
    "        nodules['coordY'] = ((nodules['coordY'] - originY)/spacingY).astype(int)\n",
    "        nodules['coordZ'] = ((nodules['coordZ'] - originZ)/spacingZ).astype(int)\n",
    "        \n",
    "        # processed image (numpy array)\n",
    "        for file in os.listdir(self.img_dir):\n",
    "            if re.search(name + '.npy$', file):\n",
    "                img = np.load(self.img_dir + 'p_' + name + '.npy')\n",
    "                break\n",
    "                \n",
    "        # convert nodules to 1-hot\n",
    "        label = np.zeros(img.shape)\n",
    "        nodules = nodules.values\n",
    "        for ind in range(nodules.shape[0]):\n",
    "            nod = nodules[ind, :]\n",
    "            label[nod[2], nod[1], nod[0]] = 1\n",
    "        \n",
    "        # convert img, label into tensors\n",
    "        return {'img': from_numpy(img), 'label': from_numpy(label)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_0 = LungsDataset('/Volumes/KaneData/processed0/meta_0.csv', \n",
    "                    '/Volumes/KaneData/candidates_V2.csv', \n",
    "                    '/Volumes/KaneData/processed0/')\n",
    "dl_0 = D.DataLoader(ds_0, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Begin training/evaluation of model.\n",
    "\n",
    "Questions: \n",
    "    - 1) How do I resize images to have same depths to normalize? \n",
    "        Final layer can just be resize to original size, and one-hot.\n",
    "    - 2) How do I deal with multiple outputs? \n",
    "        Same.\n",
    "    - 3) How should I format final layer to reflect nature of outputs?\n",
    "        Same.\n",
    "    - 4) Am I using 3d layers incorrectly? Switch to 2d? \n",
    "'''\n",
    "\n",
    "class UNetDBlock(nn.Module):\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super(UNetDBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(in_c, out_c, kernel_size = 3, padding = 1)\n",
    "        self.conv2 = nn.Conv3d(out_c, out_c, kernel_size = 3, padding = 1)\n",
    "        self.batch = nn.BatchNorm3d(out_c)\n",
    "        self.pool = nn.MaxPool3d(2)\n",
    "    def forward(self, x): \n",
    "        x = F.leaky_relu(conv1(x))\n",
    "        x = batch(x)\n",
    "        x = F.leaky_relu(conv2(x))\n",
    "        x = batch(x)\n",
    "        x = pool(x)\n",
    "        return x\n",
    "\n",
    "class UNetUBlock(nn.Module):\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super(UNetUBlock, self).__init__()\n",
    "        self.convt1 = nn.ConvTranspose3d(in_c, in_c, 2, stride = 2)\n",
    "        self.conv1 = nn.Conv3d(in_c, out_c, kernel_size = 3, padding = 1)\n",
    "        self.batch = nn.BatchNorm3d(out_c)\n",
    "        self.conv2 = nn.Conv3d(out_c, out_c, kernel_size = 3, padding = 1)\n",
    "    def forward(self, x): \n",
    "        x = F.leaky_relu(conv1(x))\n",
    "        x = batch(x)\n",
    "        x = F.leaky_relu(conv2(x))\n",
    "        x = batch(x)\n",
    "        return x\n",
    "    \n",
    "class UNetBBlock(nn.Module):\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super(UNetBBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(in_c, out_c, kernel_size = 3, padding = 1)\n",
    "        self.conv2 = nn.Conv3d(out_c, out_c, kernel_size = 3, padding = 1)\n",
    "        self.batch = nn.BatchNorm3d(out_c)\n",
    "        self.dropout = nn.Dropout3d()\n",
    "    def forward(self, x):\n",
    "        x = conv1(x)\n",
    "        x = batch(x)\n",
    "        x = dropout(x)\n",
    "        x = conv2(x)\n",
    "        x = batch(x)\n",
    "        x = dropout(x)\n",
    "        return x\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "        initf = 32\n",
    "        self.d1 = UNetDBlock(1, initf)\n",
    "        self.d2 = UNetDBlock(initf, initf * 2)\n",
    "        self.d3 = UNetDBlock(initf * 2, initf * 4)\n",
    "        self.d4 = UNetDBlock(initf * 4, initf * 8)\n",
    "        self.b = UNetBBlock(initf * 8, initf * 16)\n",
    "        self.u1 = UNetUBlock(initf * 16, initf * 8)\n",
    "        self.u2 = UNetUBlock(initf * 8, initf * 4)\n",
    "        self.u3 = UNetUBlock(initf * 4, initf * 2)\n",
    "        self.u4 = UNetUBlock(initf * 2, initf)\n",
    "        self.final = nn.Conv3d(initf, 1, kernel_size = 1)\n",
    "        self.final_sig = nn.Sigmoid()\n",
    "    def forward(self, x): # D * H * W\n",
    "        x = d1.forward(x) # D * 512 * 512 -> D/2 * 256 * 256\n",
    "        x = d2.forward(x) # D/2 * 256 * 256 -> D/4 * 128 * 128\n",
    "        x = d3.forward(x) # D/4 * 128 * 128 -> D/8 * 64 * 64\n",
    "        x = d4.forward(x) # D/8 * 64 * 64 -> D/16 * 32 * 32\n",
    "        \n",
    "        x = b.forward(x) # D/16 * 32 * 32 -> D/16 * 32 * 32\n",
    "        \n",
    "        x = u1.forward(x) # D/16 * 32 * 32 -> D/8 * 64 * 64\n",
    "        x = u2.forward(x) # D/8 * 64 * 64 -> D/4 * 128 * 128\n",
    "        x = u3.forward(x) # D/4 * 128 * 128 -> D/2 * 256 * 256\n",
    "        x = u4.forward(x) # D/2 * 256 * 256 -> D * 512 * 512\n",
    "        \n",
    "        x = final(x) # D * 512 * 512 -> D * 512 * 512 (1 channel)\n",
    "        x = final_sig(x) # D * 512 * 512 -> D * 512 * 512\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Build optimizer, loss function family.\n",
    "'''\n",
    "\n",
    "model = UNet()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Train the model.\n",
    "'''\n",
    "\n",
    "def train(model, criterion, optimizer, data_loader):\n",
    "    for epoch in range(2):\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        # currently, this code only works for batch size = 1, can change with inner for loop\n",
    "        for i_batch, batch in enumerate(data_loader): \n",
    "            tr_img = batch['img']\n",
    "            tr_label = batch['label']\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model.forward(tr_img)\n",
    "            loss = criterion(output, tr_label)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        print(\"loss for epoch of the processed batch: \" + str(running_loss / len(train_data)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
